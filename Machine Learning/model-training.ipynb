{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaviJ/Cryptocurrency-Analysis/blob/hourly-ML2/Machine%20Learning/model-training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rZnJaGTWQw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634bb293-5345-40af-af73-0be2e3dc14f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/11\n",
            "953/953 [==============================] - 17s 16ms/step - loss: 0.0165 - mean_absolute_error: 0.0527 - val_loss: 0.0016 - val_mean_absolute_error: 0.0277\n",
            "Epoch 2/11\n",
            "953/953 [==============================] - 15s 16ms/step - loss: 0.0017 - mean_absolute_error: 0.0232 - val_loss: 0.0010 - val_mean_absolute_error: 0.0209\n",
            "Epoch 3/11\n",
            "953/953 [==============================] - 15s 16ms/step - loss: 0.0014 - mean_absolute_error: 0.0209 - val_loss: 8.2410e-04 - val_mean_absolute_error: 0.0191\n",
            "Epoch 4/11\n",
            "953/953 [==============================] - 15s 16ms/step - loss: 0.0012 - mean_absolute_error: 0.0195 - val_loss: 7.0987e-04 - val_mean_absolute_error: 0.0165\n",
            "Epoch 5/11\n",
            "953/953 [==============================] - 15s 16ms/step - loss: 0.0011 - mean_absolute_error: 0.0186 - val_loss: 7.8167e-04 - val_mean_absolute_error: 0.0174\n",
            "Epoch 6/11\n",
            "953/953 [==============================] - 15s 16ms/step - loss: 0.0010 - mean_absolute_error: 0.0178 - val_loss: 0.0011 - val_mean_absolute_error: 0.0220\n",
            "Epoch 7/11\n",
            "953/953 [==============================] - 16s 16ms/step - loss: 9.6894e-04 - mean_absolute_error: 0.0177 - val_loss: 0.0010 - val_mean_absolute_error: 0.0205\n",
            "Epoch 8/11\n",
            "953/953 [==============================] - 15s 16ms/step - loss: 0.0173 - mean_absolute_error: 0.0632 - val_loss: 0.0105 - val_mean_absolute_error: 0.0813\n",
            "Epoch 9/11\n",
            "953/953 [==============================] - 15s 16ms/step - loss: 0.0308 - mean_absolute_error: 0.1196 - val_loss: 0.0087 - val_mean_absolute_error: 0.0735\n",
            "Epoch 10/11\n",
            "953/953 [==============================] - 16s 16ms/step - loss: 0.0057 - mean_absolute_error: 0.0547 - val_loss: 0.0038 - val_mean_absolute_error: 0.0470\n",
            "Epoch 11/11\n",
            "953/953 [==============================] - 15s 16ms/step - loss: 0.0043 - mean_absolute_error: 0.0446 - val_loss: 0.0027 - val_mean_absolute_error: 0.0375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_25_layer_call_fn, lstm_cell_25_layer_call_and_return_conditional_losses, lstm_cell_25_layer_call_fn, lstm_cell_25_layer_call_and_return_conditional_losses, lstm_cell_25_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: BTClstm_model\\assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: BTClstm_model\\assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000220132CA850> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/11\n",
            "953/953 [==============================] - 104s 107ms/step - loss: 0.5099 - mean_absolute_error: 0.5692 - val_loss: 0.4992 - val_mean_absolute_error: 0.5570\n",
            "Epoch 2/11\n",
            "953/953 [==============================] - 123s 129ms/step - loss: 0.5004 - mean_absolute_error: 0.5513 - val_loss: 0.4988 - val_mean_absolute_error: 0.5531\n",
            "Epoch 3/11\n",
            "953/953 [==============================] - 123s 129ms/step - loss: 0.5015 - mean_absolute_error: 0.5518 - val_loss: 0.4989 - val_mean_absolute_error: 0.5549\n",
            "Epoch 4/11\n",
            "953/953 [==============================] - 123s 129ms/step - loss: 0.4988 - mean_absolute_error: 0.5463 - val_loss: 0.4986 - val_mean_absolute_error: 0.5518\n",
            "Epoch 5/11\n",
            "369/953 [==========>...................] - ETA: 1:09 - loss: 0.4991 - mean_absolute_error: 0.5457"
          ]
        }
      ],
      "source": [
        "# Forcasting Cryptocurrency\n",
        "# Data Analysis and Visualization\n",
        "# Group 7\n",
        "\n",
        "# TensorFlow model training routine\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "# Coin ticker symbol to analyze\n",
        "symbols=['BTC','ETH','XRP','USDT','ADA','XMR','XLM','BUSD','LTC','DOGE']\n",
        "for symbol in symbols:\n",
        "    # Initialize some graphic parameters\n",
        "    mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "    mpl.rcParams['axes.grid'] = True\n",
        "\n",
        "    # Extract, transform, and load the input data\n",
        "\n",
        "    # Pull data from mongodb\n",
        "    path = tf.keras.utils.get_file(\n",
        "      origin='https://raw.githubusercontent.com/LaviJ/Cryptocurrency-Analysis/main/Data/Hourly/hourly_'+symbol+'.csv',\n",
        "      fname='hourly_'+symbol+'.csv')\n",
        "    csv_path, _ = os.path.splitext(path)\n",
        "\n",
        "    # Read data into dataframe\n",
        "    df = pd.read_csv(csv_path+\".csv\")\n",
        "\n",
        "\n",
        "    # Drop some columns\n",
        "    df = df.drop('conversionType', axis=1)\n",
        "    df = df.drop('conversionSymbol', axis=1)\n",
        "    df = df.drop('volumeto', axis=1)\n",
        "    df = df.drop('volumefrom', axis=1)\n",
        "    #df = df.drop('_id', axis=1)\n",
        "    #df = df.drop('index', axis=1)\n",
        "    #df = df.drop('Symbol', axis=1)\n",
        "    df['mid'] = (df['high'] + df ['low']) / 2\n",
        "    df = df.drop('open', axis=1)\n",
        "    df = df.drop('high', axis=1)\n",
        "    df = df.drop('low', axis=1)\n",
        "\n",
        "    # Last five years\n",
        "    df = df[:43800]\n",
        "\n",
        "    # Drop row with any zero\n",
        "    df = df[(df != 0).all(1)]\n",
        "\n",
        "    # Drop last row\n",
        "    df = df[:-1]\n",
        "\n",
        "    # Move the timestamps into an array\n",
        "    date_time =[0]\n",
        "    date_time = df.pop('time')\n",
        "\n",
        "    # Display the input data: all data and last 96 hours\n",
        "    plot_cols = ['close']\n",
        "    plot_features = df[plot_cols]\n",
        "    plot_features.index = date_time\n",
        "    _ = plot_features.plot(subplots=True)\n",
        "\n",
        "    plot_features = df[plot_cols][:96]\n",
        "    plot_features.index = date_time[:96]\n",
        "    _ = plot_features.plot(subplots=True)\n",
        "\n",
        "\n",
        "    # Show the statistical description\n",
        "\n",
        "    df.describe().transpose()\n",
        "\n",
        "    # Define sinusoid wave periods\n",
        "\n",
        "    timestamp_s = date_time \n",
        "\n",
        "    day = 24*60*60\n",
        "    year = (365.2425)*day\n",
        "    quarter = (365.2425/4)*day\n",
        "\n",
        "    # Load the waves into the dataframe\n",
        "    df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
        "    df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
        "    df['Quarter sin'] = np.sin(timestamp_s * (2 * np.pi / quarter))\n",
        "    df['Quarter cos'] = np.cos(timestamp_s * (2 * np.pi / quarter))\n",
        "    df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
        "    df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
        "\n",
        "    # Display the weekly signal\n",
        "\n",
        "    plt.plot(np.array(df['Year sin'])[:200])\n",
        "    plt.plot(np.array(df['Quarter cos'])[:200])\n",
        "    plt.plot(np.array(df['Day sin'])[:200])\n",
        "    plt.plot(np.array(df['Quarter sin'])[:200])\n",
        "    plt.xlabel('Time [h]')\n",
        "    plt.title('Time of day signal')\n",
        "\n",
        "    # Use a (70%, 20%, 10%) split for the training, validation, and test sets. \n",
        "    # Note the data is not being randomly shuffled before splitting.\n",
        "\n",
        "    column_indices = {name: i for i, name in enumerate(df.columns)}\n",
        "\n",
        "    n = len(df)\n",
        "    train_df = df[0:int(n*0.7)]\n",
        "    val_df = df[int(n*0.7):int(n*0.9)]\n",
        "    test_df = df[int(n*0.9):]\n",
        "\n",
        "    num_features = df.shape[1]\n",
        "\n",
        "    # Normalize training datasets\n",
        "\n",
        "    train_mean = train_df.mean()\n",
        "    train_std = train_df.std()\n",
        "\n",
        "    train_df = (train_df - train_mean) / train_std\n",
        "    val_df = (val_df - train_mean) / train_std\n",
        "    test_df = (test_df - train_mean) / train_std\n",
        "\n",
        "    # Show distribution\n",
        "\n",
        "    df_std = (df - train_mean) / train_std\n",
        "    df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
        "    _ = ax.set_xticklabels(df.keys(), rotation=90)\n",
        "\n",
        "    # Define TensorFlow windows\n",
        "\n",
        "    class WindowGenerator():\n",
        "      def __init__(self, input_width, label_width, shift,\n",
        "                  train_df=train_df, val_df=val_df, test_df=test_df,\n",
        "                  label_columns=None):\n",
        "        # Store the raw data.\n",
        "        self.train_df = train_df\n",
        "        self.val_df = val_df\n",
        "        self.test_df = test_df\n",
        "\n",
        "        # Work out the label column indices.\n",
        "        self.label_columns = label_columns\n",
        "        if label_columns is not None:\n",
        "          self.label_columns_indices = {name: i for i, name in\n",
        "                                        enumerate(label_columns)}\n",
        "        self.column_indices = {name: i for i, name in\n",
        "                              enumerate(train_df.columns)}\n",
        "\n",
        "        # Work out the window parameters.\n",
        "        self.input_width = input_width\n",
        "        self.label_width = label_width\n",
        "        self.shift = shift\n",
        "\n",
        "        self.total_window_size = input_width + shift\n",
        "\n",
        "        self.input_slice = slice(0, input_width)\n",
        "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "\n",
        "        self.label_start = self.total_window_size - self.label_width\n",
        "        self.labels_slice = slice(self.label_start, None)\n",
        "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
        "\n",
        "      def __repr__(self):\n",
        "        return '\\n'.join([\n",
        "            f'Total window size: {self.total_window_size}',\n",
        "            f'Input indices: {self.input_indices}',\n",
        "            f'Label indices: {self.label_indices}',\n",
        "            f'Label column name(s): {self.label_columns}'])\n",
        "        \n",
        "        # Create slices\n",
        "\n",
        "    def split_window(self, features):\n",
        "      inputs = features[:, self.input_slice, :]\n",
        "      labels = features[:, self.labels_slice, :]\n",
        "      if self.label_columns is not None:\n",
        "        labels = tf.stack(\n",
        "            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
        "            axis=-1)\n",
        "\n",
        "      # Slicing doesn't preserve static shape information, so set the shapes\n",
        "      # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
        "      inputs.set_shape([None, self.input_width, None])\n",
        "      labels.set_shape([None, self.label_width, None])\n",
        "\n",
        "      return inputs, labels\n",
        "\n",
        "    WindowGenerator.split_window = split_window\n",
        "\n",
        "    # Define a method to generate mean error plots\n",
        "\n",
        "    def plot(self, model=None, plot_col='close', max_subplots=6):\n",
        "      inputs, labels = self.example\n",
        "      plt.figure(figsize=(12, 8))\n",
        "      plot_col_index = self.column_indices[plot_col]\n",
        "      max_n = min(max_subplots, len(inputs))\n",
        "      for n in range(max_n):\n",
        "        plt.subplot(max_n, 1, n+1)\n",
        "        plt.ylabel(f'{plot_col} [normed]')\n",
        "        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
        "                label='Inputs', marker='.', zorder=-10)\n",
        "\n",
        "        if self.label_columns:\n",
        "          label_col_index = self.label_columns_indices.get(plot_col, None)\n",
        "        else:\n",
        "          label_col_index = plot_col_index\n",
        "\n",
        "        if label_col_index is None:\n",
        "          continue\n",
        "\n",
        "        plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
        "                    edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
        "        if model is not None:\n",
        "          predictions = model(inputs)\n",
        "          plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
        "                      marker='X', edgecolors='k', label='Predictions',\n",
        "                      c='#ff7f0e', s=64)\n",
        "\n",
        "        if n == 0:\n",
        "          plt.legend()\n",
        "\n",
        "      plt.xlabel('Time [h]')\n",
        "\n",
        "    WindowGenerator.plot = plot\n",
        "\n",
        "    #  This make_dataset method will take a time series DataFrame and convert\n",
        "    #  it to a tf.data.Dataset of (input_window, label_window) pairs using the \n",
        "    #  tf.keras.utils.timeseries_dataset_from_array function\n",
        "\n",
        "    def make_dataset(self, data):\n",
        "      data = np.array(data, dtype=np.float32)\n",
        "      ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "          data=data,\n",
        "          targets=None,\n",
        "          sequence_length=self.total_window_size,\n",
        "          sequence_stride=1,\n",
        "          shuffle=True,\n",
        "          batch_size=32,)\n",
        "\n",
        "      ds = ds.map(self.split_window)\n",
        "\n",
        "      return ds\n",
        "\n",
        "    # The WindowGenerator object holds training, validation, and test data.\n",
        "    # accessing them as tf.data.Datasets using the make_dataset method as\n",
        "    # defined earlier\n",
        "    WindowGenerator.make_dataset = make_dataset\n",
        "    @property\n",
        "    def train(self):\n",
        "      return self.make_dataset(self.train_df)\n",
        "\n",
        "    @property\n",
        "    def val(self):\n",
        "      return self.make_dataset(self.val_df)\n",
        "\n",
        "    @property\n",
        "    def test(self):\n",
        "      return self.make_dataset(self.test_df)\n",
        "\n",
        "    @property\n",
        "    def example(self):\n",
        "      \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
        "      result = getattr(self, '_example', None)\n",
        "      if result is None:\n",
        "        # No example batch was found, so get one from the `.train` dataset\n",
        "        result = next(iter(self.train))\n",
        "        # And cache it for next time\n",
        "        self._example = result\n",
        "      return result\n",
        "\n",
        "    WindowGenerator.train = train\n",
        "    WindowGenerator.val = val\n",
        "    WindowGenerator.test = test\n",
        "    WindowGenerator.example = example\n",
        "\n",
        "    # Define the training procedure\n",
        "\n",
        "    MAX_EPOCHS = 11\n",
        "    BATCH_SIZE = 1024\n",
        "    def compile_and_fit(model, window, patience=7):\n",
        "      early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                        patience=patience,\n",
        "                                                        mode='min',\n",
        "                                                        restore_best_weights=True)\n",
        "      opt = tf.keras.optimizers.Adam(learning_rate=.01)\n",
        "      opt = tf.keras.mixed_precision.LossScaleOptimizer(opt)\n",
        "      model.compile(loss=tf.losses.MeanSquaredError(),\n",
        "                    optimizer=opt,\n",
        "                    metrics=[tf.metrics.MeanAbsoluteError()])\n",
        "\n",
        "      with tf.device('/device:GPU:1'):\n",
        "        history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
        "                          validation_data=window.val,\n",
        "                          callbacks=[early_stopping],batch_size=BATCH_SIZE)\n",
        "      return history\n",
        "\n",
        "    # Create a model to learn to predict 96 hours into the future, given 96 hours\n",
        "    # of the past.\n",
        "\n",
        "    OUT_STEPS = 96\n",
        "    multi_window = WindowGenerator(input_width=96,\n",
        "                                  label_width=OUT_STEPS,\n",
        "                                  shift=OUT_STEPS)\n",
        "\n",
        "    multi_window.plot()\n",
        "    multi_window\n",
        "\n",
        "    # Calculate the Recurrent model\n",
        "\n",
        "    multi_lstm_model = tf.keras.Sequential([\n",
        "        # Shape [batch, time, features] => [batch, lstm_units].\n",
        "        # Adding more `lstm_units` just overfits more quickly.\n",
        "        tf.keras.layers.LSTM(32, return_sequences=False),\n",
        "        # Shape => [batch, out_steps*features].\n",
        "        tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
        "                              kernel_initializer=tf.initializers.zeros()),\n",
        "        # Shape => [batch, out_steps, features].\n",
        "        tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
        "    ])\n",
        "\n",
        "    history = compile_and_fit(multi_lstm_model, multi_window)\n",
        "    multi_window.plot(multi_lstm_model)\n",
        "    multi_lstm_model.save(symbol+\"lstm_model\")\n",
        "\n",
        "    # Long Short-Term Memory Bi-Directional NN\n",
        "      \n",
        "    # Calculate the Bidirectional model\n",
        "    bidir = tf.keras.Sequential()\n",
        "    bidir.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM((256), return_sequences=True)))\n",
        "    bidir.add(tf.keras.layers.Dropout(rate=.5))\n",
        "    bidir.add(tf.keras.layers.Dense(units=8, activation='sigmoid'))\n",
        "\n",
        "    history = compile_and_fit(bidir, multi_window)\n",
        "    multi_window.plot(bidir)\n",
        "    bidir.save(symbol+\"bidir_model\")\n",
        "\n",
        "    # Define the Autoregressive RNN LSTM model\n",
        "    class FeedBack(tf.keras.Model):\n",
        "      def __init__(self, units, out_steps):\n",
        "        super().__init__()\n",
        "        self.out_steps = out_steps\n",
        "        self.units = units\n",
        "        self.lstm_cell = tf.keras.layers.LSTMCell(units)\n",
        "        # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
        "        self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(num_features)\n",
        "\n",
        "    feedback_model = FeedBack(units=32, out_steps=OUT_STEPS)\n",
        "\n",
        "    # A warmup method to initialize internal state based on the inputs.\n",
        "    \n",
        "    def warmup(self, inputs):\n",
        "      # inputs.shape => (batch, time, features)\n",
        "      # x.shape => (batch, lstm_units)\n",
        "      x, *state = self.lstm_rnn(inputs)\n",
        "\n",
        "      # predictions.shape => (batch, features)\n",
        "      prediction = self.dense(x)\n",
        "      return prediction, state\n",
        "\n",
        "    FeedBack.warmup = warmup\n",
        "\n",
        "    # Display the tesor shape\n",
        "    prediction, state = feedback_model.warmup(multi_window.example[0])\n",
        "    prediction.shape\n",
        "\n",
        "    # Define the feedback method\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "      # Use a TensorArray to capture dynamically unrolled outputs.\n",
        "      predictions = []\n",
        "      # Initialize the LSTM state.\n",
        "      prediction, state = self.warmup(inputs)\n",
        "\n",
        "      # Insert the first prediction.\n",
        "      predictions.append(prediction)\n",
        "\n",
        "      # Run the rest of the prediction steps.\n",
        "      for n in range(1, self.out_steps):\n",
        "        # Use the last prediction as input.\n",
        "        x = prediction\n",
        "        # Execute one lstm step.\n",
        "        x, state = self.lstm_cell(x, states=state,\n",
        "                                  training=training)\n",
        "        # Convert the lstm output to a prediction.\n",
        "        prediction = self.dense(x)\n",
        "        # Add the prediction to the output.\n",
        "        predictions.append(prediction)\n",
        "\n",
        "      # predictions.shape => (time, batch, features)\n",
        "      predictions = tf.stack(predictions)\n",
        "      # predictions.shape => (batch, time, features)\n",
        "      predictions = tf.transpose(predictions, [1, 0, 2])\n",
        "      return predictions\n",
        "\n",
        "    FeedBack.call = call\n",
        "\n",
        "    # Print the output shape\n",
        "\n",
        "    print('Output shape (batch, time, features): ', feedback_model(multi_window.example[0]).shape)\n",
        "\n",
        "    # Train the Autoregressive RNN feedback model \n",
        "    history = compile_and_fit(feedback_model, multi_window)\n",
        "    multi_window.plot(feedback_model)\n",
        "    # Save the feedback_model\n",
        "    feedback_model.save(symbol+'feedback_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Portions of code adapted/refactored from Tensorflow Routines for Time Series Data\n",
        "https://www.tensorflow.org/tutorials/structured_data/time_series\n",
        "````\n",
        "  Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,\n",
        "Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis,\n",
        "Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,\n",
        "Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia,\n",
        "Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster,\n",
        "Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens,\n",
        "Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,\n",
        "Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas,\n",
        "Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,\n",
        "Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on\n",
        "heterogeneous systems, 2015. Software available from tensorflow.org.\n",
        "````"
      ],
      "metadata": {
        "id": "A6tLrn2_Oefm"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NCKS4m1VKrDQ",
        "icsBAjCzMaMl"
      ],
      "name": "Copy of model-training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}